{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\anaconda\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow-gpu in c:\\anaconda\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: sklearn in c:\\anaconda\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\anaconda\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\anaconda\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\anaconda\\lib\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\anaconda\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\anaconda\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tensorflow-gpu pandas matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "# Data preprocessing################################\n",
    "# Remove useless words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Aggressive conversion of words to base form\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# Import packages that help us to create document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# Data preprocessing################################\n",
    "\n",
    "# Data visualization################################\n",
    "\n",
    "# For creating graphs\n",
    "from matplotlib import pyplot as plt\n",
    "# Data visualization################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Setting the dtype policy to utilize mixed precision\n",
    "# This will use the tensor cores of the GPU\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools to split data and evaluate model performance\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Import ML algos\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_colab\\\\train.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('train_colab', 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('train_colab', 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TextComment</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>SevereToxicity</th>\n",
       "      <th>ObsceneLanguage</th>\n",
       "      <th>Insult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>go back to your country smelly arab</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>RUSSIAN WHY U MAD BITCH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>china teammate so toxic</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                        TextComment  Toxic  \\\n",
       "0  0000997932d777bf                go back to your country smelly arab      0   \n",
       "1  000103f0d9cfb60f                            RUSSIAN WHY U MAD BITCH      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e                            china teammate so toxic      1   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   SevereToxicity  ObsceneLanguage  Insult  \n",
       "0               1                0       1  \n",
       "1               1                1       1  \n",
       "2               0                0       0  \n",
       "3               0                0       1  \n",
       "4               0                0       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'china teammate so toxic'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3]['TextComment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic              1\n",
       "SevereToxicity     0\n",
       "ObsceneLanguage    0\n",
       "Insult             1\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[2:]].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset overview\n",
    "The overview will conclude a discussion with the dataset using exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128606"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show rows of the dataset\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.889655\n",
       "1    0.110345\n",
       "Name: Toxic, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check percentage of comments that are toxic compared to normal comments\n",
    "df.Toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This states that approximately n% of the dataset is toxic compared to normal comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic              14191\n",
       "SevereToxicity      1597\n",
       "ObsceneLanguage     7889\n",
       "Insult              7499\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new subset of the data by only taking the 2nd column onwards (comments and categories)\n",
    "df_count=df.iloc[:,2:].sum()\n",
    "\n",
    "# Print dataset summary\n",
    "df_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Toxic', 'SevereToxicity', 'ObsceneLanguage', 'Insult'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_count.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAAFrCAYAAABcwrnQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2S0lEQVR4nO3de7wVdb3/8dcbtndFRcAQxK2JFoKQoimmmdqRLl4qPUIXUSnLzNQsj3ZRu5BWFmYlx2ugGYhawTE1OSj6S03cKIqgJAJHCBTwriiKfn5/fL+bFou1YRbsvdcG3s/HYz3WrM98vzPfmdl7fdbMfGdGEYGZmZmtXrtaN8DMzGx94IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATpplZJikkTap1O6xtcsK0mpL0AUm/kfSEpFckvS1pgaS/ShoqafNat3F9JuminAQOrXVbzNZ3dbVugG28JF0AXEj64fYPYBTwOrAjcChwDXAa0L9GTTQzW8EJ02pC0neBHwLzgOMj4qEKZT4NnNPabTMzq8SHZK3VSaoHLgLeAT5ZKVkCRMRtwMAK9f9T0n35EO6bkqZJOl/SZhXKzs2vrSUNlzQv15kq6dhcpk7SdyU9LektSc9I+kaFaR2aD29eJKm/pDtzG16SdKuknXO53SSNkbQ4z+seSX2bWBdb5rZPlfSGpNclPShp8Brm3y8ftn5Z0lJJ90oaUL7spD14gHty3ZAUJWV2lHSppJl5/i/n4ZGSdqvU5tWs420l/VbSv/J6nCHpm5LURL0PS7pF0nP5UPw8SVdK2qlC2Um57ZtKuiC3cZmkkQXb+AFJ1+V2LpO0SNL/k3Ragbo75XneX9LWBZL+KOmDTdQ5WtJESQvz/BbkbfT1snK7SbpK0qz8t/Ji/nv+b0k7FFk2az3yvWSttUn6IXABMCYiVkkMa6j7U+B8YAlwC+kQ7ieAvYB7gY9HxDsl5ecCmwDPAh2BCcCmwGBgS+A/gK8DHwbuAJYBxwNdgEERcVPJtA4F7gFuBw7L83sC6JOn8zRwNPB34CngIWAX4LO5vbtFxOsl09sOuBv4EPAI8ADpR+yRwPuBYRHx/Qrz/2ue/4PAo0AP4HPA20C/iJiZy58FHAt8lHS4e27jtCLiIklbAo/neU3Iw8ptPhz4Uv7Rslp5HW8K/AvYDhifP38O6ApcERGnl9U5GbiatL7Hk4409Mzr73nggIh4tqT8pLwctwH7kbbVImBRRPxyDe37FHAzsBlwZ17O7YC+QNeI2LWkbAD3RsShJbFBwHWkdT+X9DfXE/g0aZ0fFBGPlZQ/FbgSeA74H9K27wLsTfrO3S+X60r6++lA+pt6Ctgc2JW0/j8cEU+sbtmslUWEX3616guYCATw5SrrHZjrPQu8ryReR/piCuC7ZXXm5vj/AJuVxA/O8ReBh4HtSsbtRvoifLRsWofmOgF8oWzctSXT+17ZuB/kcWeWxUfm+Lll8c1JX+zvkRJgpfmfVFbnqzl+RVn8ohw/tML6PCqPG15h3KbANgW3S+M6/nvZOu4IPJPHHVIS3yOv31lAt7JpHQa8C/y5LD4pT+dxoFMVfzOdgFfy/D5aYXz3ss8BTCqLdam0LkgJ93XgjrL4FNIPgS6V2lMyfEalv4s8bitgi3X5P/Or+V8+JGu10DW/z6+y3in5/ScR8VxjMCKWk851vgd8uYm6Z0XEspI6/w+YA2wP/FdEvFwybjZwP9BHUvsK0/p7RNxYFhuV318BLikbd31+79cYyIfbvgg0RMTPSwtHxFvAf5H29j5fYf73R8TIsth1wHJg/wrl1+TN8kBEvB0Rr1U5nfPL1vGLwI/zx5NLyp1G2us/MyL+VTbfu0l7nEdJ2qbCPH4QEUuqaNMQ0h7ciIi4t3xkRKzxbzAiFlVaF5H2Ku8GPiZpk7LRy0mnHMrrVGp7pfX/RkSsErfacqcfq4XGc1rVng/YJ7/fXT4iIv4paT6wq6TtShMg8HJEPFNhegtIh7+mVBj3L6A98L48XKqhiWkBTI2IdytMC6B7SWy/PP2QdFGF6TV+AVc6R7bK/CPiHUnPk34AFHVvbtt5kvYhHRa8n8rLsCbLSYeUy03K7x8qiR2Y3z8qab8KdbqQ1s0erLptJlfZrgPy+x1V1ltJPqz7NVKP7U6s+t3ZCViYh28EfglMl3QTaT3fHxGLy+qMB34K/E7SkcDfSOt/RkT4XFkb5IRptbAA+AArJ5Aits3vC5sYv5B0Pm9b4OWS+CtNlF8OEBGVxi/P7+V7Dk1Nb3lT4yJiee73Ujqtxg4d++VXU7auEHu5ibLLSYmmkIh4VdIBpN7KR5POnQIskXQFaU9+lb2kJixpIsk2HgnYtiTWuOzfWcM0Ky37cxViq7Ndfi//0VOYpG8CvwZeIp3rfRZYSvrBdyzp0OyKDmcR8StJS0jnxr8JnEX6YXQv8J2IaMjl/k/S/qTD5gNJ57oB5km6NCIuX9s2W8twwrRa+DvpXNXhpHN/RTUmo/eRzo2V61pWri1rbOPwiPhWrRqRD0kOzT1Ze5G2y+mkTlntSOdfi+gkqX2FpPm+/F66TRqHt42IV6tsb7V7Xi/n927AtCrrIqmO9IPiOWCfiFhYNv7ASvUi4nrg+tyxawDwGdIphb9J+mBELMrlngROyPPpCxxBOrf5a0lvREQ1/x/WwnwO02rh96TzO5+T1Gt1BbXypSKP5vdDK5TbnbTHOqfscGxbNZl0zvXgFp5PYwJb7Z5nJNMj4jfAx3P42CrmU0dKDOUOze+PlsT+kd9betlL5/WJtazfibSX+kCFZLk1/z5NUFFEvBwRt0fEV0idvDpSYbkjYnlETImIn5F6cEN1699agROmtbqImEs6DLUp8FdJFe/kI2kgK597ui6/f19S55Jy7YFLSX/P68Uv8ryHcSPQX9IP8h7GSiS9X9Kuq9auygv5vUeF6fdWuia23I75fWmV87q49AeOpI5A42Uxvy8p91vSD6bhkvao0K5NJTVXMh0FvAqcJumQCvNa02mBRaT1sG9OkI31NiEdpu1UYZoDK21P0rlZ8vSQtL+kHSuUW9v1by3Mh2StJiLip/lL5ULgYUkPkDqzNN4a7xDStW4NJXUekPRz4FzgCUm3AG+Q9h56kw71/qJVF2TdfIO0jD8CviTp76RrEHcidfbZj7S3MWcd5nEPaU/2Ykm9SefhiIifkA7//Sqv+6dIyaE7cEyuU826XEg6j/eEpPGk87XH8e/rMO9rLBgRT0k6hfQDaLqkO4F/5jo9SHtgi0nnuddJRCyR9HnSNbv3SLqDdGlKB9J1kTuTOn41Vf89SZcD5wHTJI0j/dD7GGlv8Z48XGoM8FbennNJndwOJm3PKcD/5nKfB07P5zZnkbbN+0mX+ywDLluXZbcWUOvrWvzauF+kxPAb0gXcr5Kul1tI2rMcSsl1fSV1BpGS42vAW8B04HvA5hXKzgXmNjHvSeTTYhXGjSR16qgviR2aYxdVKF+fx41sYnqrXN+X45uSEucDpHN7y0idSiaSOovsUGT+q1tW0uUrU0mXLwT/PhX4QeBXpB8li/O855KSy4AqtuHc/NoW+B2pg80y4ElSpxc1Ua9PXs//l8u/mP8OrgQOK7qtCrZxL9LlPf/Kf2PPk3qvnrqm7UTasfgWMCOvw+eAG0g3eKj0d/I14M/AbNJe4oukQ9LnUnI9J+lmGSOAx3KZN0mJ8/dA71r/b/q16st3+jGzdZLv9ENE1Ne2JWYty+cwzczMCnDCNDMzK8AJ08zMrACfwzQzMyvAe5hmZmYFOGGamZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlaAE6aZmVkBTphmZmYFOGGamZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlZAXa0bUEudOnWK+vr6WjfDzMzakClTpiyJiM7l8Y06YdbX19PQ0FDrZpiZWRsi6f8qxX1I1szMrAAnzDbglFNOoUuXLvTu3XuVcZdeeimSWLJkCQAvvPACH/vYx9h66635xje+sVLZm266ib333pu99tqLc889d0X8vvvuY5999qGuro5bbrmlZRfGzGwD5YTZBpx00knceeedq8TnzZvHhAkT6NGjx4rY5ptvzo9//GMuvfTSlcq+8MILfOc732HixIlMnz6d559/nokTJwLQo0cPRo4cyec///mWXRAzsw2YE2YbcMghh9CxY8dV4meffTY///nPkbQittVWW/GRj3yEzTfffKWys2fPZo899qBz53Se+ogjjuDWW28F0rnavffem3btvLnNzNaWv0HbqPHjx9OtWzf69u1bqPzuu+/OU089xdy5c1m+fDl/+ctfmDdvXgu30sxs47FR95Jtq5YuXcqwYcO46667CtfZfvvtGTFiBCeccALt2rVjwIABzJ49uwVbaWa2cfEeZhv0zDPPMGfOHPr27Ut9fT3z589nn3324bnnnlttvaOOOoqHHnqIBx98kD333JOePXu2UovNzDZ83sNsg/r06cOiRYtWfG68XrRTp06rrbdo0SK6dOnCSy+9xBVXXMHYsWNbuqlmZhsN72G2AYMHD+bAAw9k5syZdO/enWuvvXa15evr6/nWt77FyJEj6d69OzNmzADgzDPPpFevXhx00EGcd9557LHHHgA8/PDDdO/enZtvvpmvfvWr7LXXXi2+TGZmGxpFRK3bUDP9+/cP3+nHzMxKSZoSEf3L497DNDMzK6BVzmFKug74NLAoInqXjfs28Augc0QsybHzgaHAu8A3I+JvOb4vMBLYArgdODMiQtJmwPXAvsALwAkRMbe52r/vd65vrkltVKb84sRaN8HMrNm01h7mSGBgeVDSzsDHgWdLYr2AQcBeuc4Vktrn0SOAU4Ge+dU4zaHASxGxOzAc+FmLLIWZmW20WiVhRsR9wIsVRg0HzgVKT6QeA4yJiGURMQeYBewvqSvQISIejHTi9Xrg2JI6o/LwLcDhKr09jpmZ2Tqq2TlMSUcD/4qIx8pGdQNKb1EzP8e65eHy+Ep1ImI58AqwQxPzPVVSg6SGxYsXr/NymJnZxqEmCVPSlsD3gAsqja4Qi9XEV1dn1WDEVRHRPyL6N9531czMbE1qtYf5fmBX4DFJc4HuwCOS3kfac9y5pGx3YEGOd68Qp7SOpDpgWyofAjYzM1srNUmYETEtIrpERH1E1JMS3j4R8RwwHhgkaTNJu5I690yOiIXAa5IOyOcnTwTG5UmOB4bk4eOAu2NjvsDUzMyaXaskTEmjgQeBPSXNlzS0qbIRMR0YC8wA7gROj4h38+jTgGtIHYGeAe7I8WuBHSTNAr4FnNciC2JmZhutVrkOMyIGr2F8fdnnYcCwCuUagN4V4m8Bx69bK83MzJrmO/2YmZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlaAE6aZmVkBTphmZmYFOGGamZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlaAE6aZmVkBTphmZmYFOGGamZkV4IRpZmZWgBOmmZlZAa2SMCVdJ2mRpCdKYr+Q9JSkxyX9WdJ2JePOlzRL0kxJR5bE95U0LY+7XJJyfDNJN+X4Q5LqW2O5zMxs49Fae5gjgYFlsQlA74jYG/gncD6ApF7AIGCvXOcKSe1znRHAqUDP/Gqc5lDgpYjYHRgO/KzFlsTMzDZKrZIwI+I+4MWy2F0RsTx//AfQPQ8fA4yJiGURMQeYBewvqSvQISIejIgArgeOLakzKg/fAhzeuPdpZmbWHNrKOcxTgDvycDdgXsm4+TnWLQ+Xx1eqk5PwK8AOLdheMzPbyNQ8YUr6HrAcuLExVKFYrCa+ujqV5neqpAZJDYsXL662uWZmtpGqacKUNAT4NPCFfJgV0p7jziXFugMLcrx7hfhKdSTVAdtSdgi4UURcFRH9I6J/586dm2tRzMxsA1ezhClpIPBfwNERsbRk1HhgUO75uiupc8/kiFgIvCbpgHx+8kRgXEmdIXn4OODukgRsZma2zupaYyaSRgOHAp0kzQcuJPWK3QyYkPvn/CMivhYR0yWNBWaQDtWeHhHv5kmdRupxuwXpnGfjec9rgRskzSLtWQ5qjeUyM7ONR6skzIgYXCF87WrKDwOGVYg3AL0rxN8Cjl+XNpqZma1OzTv9mJmZrQ+cMM3MzApwwjQzMyvACdPMzKwAJ0wzM7MCnDDNzMwKcMI0MzMrwAnTzMysACdMMzOzApwwzczMCnDCNDMzK8AJ08zMrAAnTDMzswKcMM3MzApwwjQzMyvACdPMzKwAJ0wzM7MCnDDNzMwKcMI0MzMrwAnTzMysACdMMzOzApwwzczMCnDCNDMzK8AJ08zMrAAnTDMzswKcMM3MzApwwjQzMyvACdPMzKyAVkmYkq6TtEjSEyWxjpImSHo6v29fMu58SbMkzZR0ZEl8X0nT8rjLJSnHN5N0U44/JKm+NZbLzMw2Hq21hzkSGFgWOw+YGBE9gYn5M5J6AYOAvXKdKyS1z3VGAKcCPfOrcZpDgZciYndgOPCzFlsSMzPbKLVKwoyI+4AXy8LHAKPy8Cjg2JL4mIhYFhFzgFnA/pK6Ah0i4sGICOD6sjqN07oFOLxx79PMzKw51PIc5o4RsRAgv3fJ8W7AvJJy83OsWx4uj69UJyKWA68AO1SaqaRTJTVIali8eHEzLYqZmW3o2mKnn0p7hrGa+OrqrBqMuCoi+kdE/86dO69lE83MbGNTy4T5fD7MSn5flOPzgZ1LynUHFuR49wrxlepIqgO2ZdVDwGZmZmutlglzPDAkDw8BxpXEB+Wer7uSOvdMzodtX5N0QD4/eWJZncZpHQfcnc9zmpmZNYu61piJpNHAoUAnSfOBC4FLgLGShgLPAscDRMR0SWOBGcBy4PSIeDdP6jRSj9stgDvyC+Ba4AZJs0h7loNaYbHMzGwj0ioJMyIGNzHq8CbKDwOGVYg3AL0rxN8iJ1wzM7OW0BY7/ZiZmbU5TphmZmYFOGGamZkV4IRpZmZWgBOmmZlZAWudMCVtIWnT5myMmZlZW1U4YUq6VNL+efhTpOsdX5Z0VEs1zszMrK2oZg/zC0Dj8ywvAL4IHA38tLkbZWZm1tZUc+OCLSNiqaQdgN0i4lYASbu0TNPMzMzajmoS5j8lfQHYHZgAIKkT8GZLNMzMzKwtqSZhfh34NfAOcEqOHQnc1dyNMjMza2sKJ8yIeBgYUBa7EbixuRtlZmbW1lR1WYmkj0u6VtL/5M/9JR3WMk0zMzNrO6q5rOQMYATwNHBIDr8J/KQF2mVmZtamVLOHeRZwRERcAryXY08BezZ3o8zMzNqaahLmNsC8PBz5fRPg7WZtkZmZWRtUTcK8DzivLPZN4J7ma46ZmVnbVM1lJWcA/yPpK8A2kmYCrwK+NZ6ZmW3wqrmsZKGk/YD9gF1Ih2cnR8R7q69pZma2/iucMCX1A16IiMnA5BzbWVLHiHishdpnZmbWJlRzDvMPpE4+pTYFbmi+5piZmbVN1STMHhExuzQQEc8A9c3aIjMzszaomoQ5X9I+pYH8eUHzNsnMzKztqSZhDgfGSTpD0ifznX/+DPyqZZpmZhuzmTNn0q9fvxWvDh06cNlllzF16lQOOOAA+vXrR//+/Zk8eTIA77zzDkOGDKFPnz588IMf5OKLL14xrZtuuom9996bvfbai3PPPbdWi2TruWp6yV4t6WVgKLAzqZfsORFxSwu1zcw2YnvuuSdTp04F4N1336Vbt2585jOf4Stf+QoXXnghn/jEJ7j99ts599xzmTRpEjfffDPLli1j2rRpLF26lF69ejF48GC22WYbvvOd7zBlyhQ6d+7MkCFDmDhxIocffnhtF9DWO1XdfD0ibo6IgRGxV353sjSzFjdx4kTe//73s8suuyCJV199FYBXXnmFnXbaCQBJvPHGGyxfvpw333yTTTfdlA4dOjB79mz22GMPOnfuDMARRxzBrbfeWrNlsfVXNTcuQNJ/AP2ArUvjEXFBM7bJzGwlY8aMYfDgwQBcdtllHHnkkXz729/mvffe44EHHgDguOOOY9y4cXTt2pWlS5cyfPhwOnbsiCSeeuop5s6dS/fu3fnLX/7C22/7jp5WvWqeVvJb0qUl+5IOyTa+uq9LAySdLWm6pCckjZa0uaSOkiZIejq/b19S/nxJsyTNlHRkSXxfSdPyuMslaV3aZWZtw9tvv8348eM5/vjjARgxYgTDhw9n3rx5DB8+nKFDhwIwefJk2rdvz4IFC5gzZw6//OUvmT17Nttvvz0jRozghBNO4OCDD6a+vp66uqr2FcyA6g7JDgb2jYgTIuLkktcpaztzSd1I96PtHxG9gfbAINI9aydGRE9gYv6MpF55/F7AQOAKSe3z5EYApwI982vg2rbLzNqOO+64g3322Ycdd9wRgFGjRvHZz34WgOOPP35Fp58//vGPDBw4kE022YQuXbpw0EEH0dDQAMBRRx3FQw89xIMPPsiee+5Jz549a7Mwtl6rJmG+ALzcAm2oA7aQVAdsSbpM5RhgVB4/Cjg2Dx8DjImIZRExB5gF7C+pK9AhIh6MiACuL6ljZuux0aNHrzgcC7DTTjtx7733AnD33XevSH49evTg7rvvJiJ44403+Mc//sEHPvABABYtWgTASy+9xBVXXMGXv/zlVl4K2xBUkzB/Cdwo6UBJu5W+1nbmEfEv4FLgWWAh8EpE3AXsGBELc5mFQJdcpRv/fsQYwPwc65aHy+Nmth5bunQpEyZMWLFHCXD11Vdzzjnn0LdvX7773e9y1VVXAXD66afz+uuv07t3b/bbbz9OPvlk9t57bwDOPPNMevXqxUEHHcR5553HHnvsUZPlWR81dXlPo0svvRRJLFmyBEiH0E8++WT69OlD3759mTRp0oqy6/vlPdUcyB+R3z9dFg/SodSq5XOTxwC7kvZeb5b0xdVVqRCL1cQrzfNU0qFbevToUU1zzayVbbnllrzwwgsrxT7ykY8wZcqUVcpuvfXW3HzzzRWnM3r06BZp38agqct7AObNm8eECRNW+i69+uqrAZg2bRqLFi3iE5/4BA8//DAvvfTSen95T+E9zIho18RrrZJldgQwJyIWR8Q7wJ+AAcDz+TAr+X1RLj+f1NGoUXfSIdz5rNz5qDFeaTmuioj+EdG/sZu5mZmtWenlPQBnn302P//5zyntYzljxowVSbBLly5st912NDQ0bBCX91TdVUzSzkC3iPhHM8z/WeAASVsCbwKHAw3AG8AQ4JL8Pi6XHw/8UdKvgJ1InXsmR8S7kl6TdADwEHAi8JtmaJ+ZlTjoNwfVugnrpfvPuL/WTWgWpZf3jB8/nm7dutG3b9+VyvTt25dx48YxaNAg5s2bx5QpU5g3bx6HHXbYen95TzWP9+oBjCZdhxnA1pKOAwZGxFqdQY+IhyTdAjwCLAceBa4iXec5VtJQUlI9PpefLmksMCOXPz0i3s2TOw0YCWwB3JFfZmbWDBov77n44otZunQpw4YN46677lql3CmnnMKTTz5J//792WWXXRgwYAB1dXUrXd7Trl07BgwYwOzZsyvMqe2qZg/zSuCvwMGkHrMAE0idgdZaRFwIXFgWXkba26xUfhgwrEK8Aei9Lm0xM7PKSi/vmTZtGnPmzFmxdzl//nz22WcfJk+ezPve9z6GDx++ot6AAQNW9GQ+6qijOOqoowC46qqraN9+Xc7otb5qEub+wKci4j1JARARr0jatmWaZmZmbUXp5T19+vRZcakOQH19PQ0NDXTq1ImlS5cSEWy11VZMmDCBuro6evXqBaTLe7p06bLi8p6xY8fWZFnWVjUJ83lgd+CfjYF8I4Fnm7tRZmbWdjRe3nPllVeuseyiRYs48sgjadeuHd26deOGG25YMe7MM8/kscceA+CCCy5Y7y7vqSZhXgrcJulioE7SYOC7pI45Zma2gap0eU+puXPnrhiur69n5syZFcut75f3VPN4r+skvUi6hnEeqSfqDyLiLy3UNjMzszajUMLM92udCBzpBGlmVlv3HvLRWjdhvfTR++5dp/qFblyQL93YtWh5MzOzDU01CfCHwAhJu0hqL6ld46ulGmdmZtZWVNPp55r8/qWSmFiHe8mamZmtL6pJmD1Jd9cxMzPb6FTT6ecJYLuIWNayTTIzM2t7qun0809gh5ZtjpmZWdtUzSHZG0k3Lvg16XFaK543GRF3N3fDzMzM2pJqEuZp+f2isngAuzVLa8zMzNqoau70s2tLNsTMzKwt8zWUZmZmBVTzAOl5lJy3LBURPZqtRWZmZm1QNecwv1j2uStwJjCm+ZpjZmbWNlVzDnOVu9ZKmgTcCfy6GdtkZmbW5qzrOcxlpJuym5mZbdCqOYf5o7LQlsAngTuatUVmZmZtUDXnMHcu+/wG8CvghuZrjpmZWdtUzTnMk1uyIWZmZm1Z4XOYks6TtF9ZbH9J5zZ/s8zMzNqWajr9nAnMKIvNAM5qttaYmZm1UdUkzE2Bd8pibwObN19zzMzM2qZqEuYU4Otlsa8BjzRfc8zMzNqmanrJng1MkPQl4Blgd2BH4OMt0TAzM7O2pJpestMl7QF8mnSJyZ+A2yLi9ZZqnJmZWVtRTS/ZbsAmETEmIn4REWOATSTttC4NkLSdpFskPSXpSUkHSuooaYKkp/P79iXlz5c0S9JMSUeWxPeVNC2Pu1yS1qVdZmZmpao5h/kXoHtZrDvw53Vsw6+BOyPiA0Bf4EngPGBiRPQEJubPSOoFDAL2AgYCV0hqn6czAjgV6JlfA9exXWZmZitUkzD3iIhppYH8+QNrO3NJHYBDgGvz9N6OiJeBY4BRudgo4Ng8fAwwJiKWRcQcYBawv6SuQIeIeDAiAri+pI6Zmdk6qyZhLpa0e2kgf35hHea/G7AY+L2kRyVdI2krYMeIWAiQ37vk8t2AeSX15+dYtzxcHl+FpFMlNUhqWLx48To03czMNibVJMzrgFslHSWpl6SjgFuAa9Zh/nXAPsCIiPgQ6f60562mfKXzkrGa+KrBiKsion9E9O/cuXO17TUzs41UNZeVXEK6UcEvSOcu55EOpf5qHeY/H5gfEQ/lz7eQEubzkrpGxMJ8uHVRSfnSm8B3BxbkePcKcTMzs2ZRaA9TUh1wIvAh4FlgPClx/joi3lvbmUfEc8A8SXvm0OGk2+2NB4bk2BBgXB4eDwyStJmkXUmdeybnw7avSTog9449saSOmZnZOlvjHqakbYEJwC6kZ19OAboCFwOnSToiIl5ZhzacAdwoaVNgNnAyKZGPlTSUlKCPhxXXgo4lJdXlwOkR8W6ezmnASGCL3E4/p9PMzJpNkUOyF5M65nwsIt5oDObOOWPz+PJb5hUWEVOB/hVGHd5E+WHAsArxBqD32rbDzMxsdYockj0WOK00WQLkz6cDn2mBdpmZmbUpRRLmtsC/mhg3H+jQfM0xMzNrm4okzGeAw5oYdzjpvKOZmdkGrUjC/BVwvaTPSWoHIKmdpONInWzW5bISMzOz9cIaO/1ExEhJO5CS42hJS4BOwDLgRxHx+5ZtopmZWe0VunFBRPxS0lXAAFKyXAI8GBGvtmTjzMzM2opqnof5GvC3FmyLmZlZm1XNvWTNzMw2Wk6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlaAE6aZmVkBTphmZmYFOGGamZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlZAm0iYktpLelTSbflzR0kTJD2d37cvKXu+pFmSZko6siS+r6RpedzlklSLZTEzsw1Tm0iYwJnAkyWfzwMmRkRPYGL+jKRewCBgL2AgcIWk9rnOCOBUoGd+DWydppuZ2cag5glTUnfgU8A1JeFjgFF5eBRwbEl8TEQsi4g5wCxgf0ldgQ4R8WBEBHB9SR0zM7N1VvOECVwGnAu8VxLbMSIWAuT3LjneDZhXUm5+jnXLw+VxMzOzZlHThCnp08CiiJhStEqFWKwmXmmep0pqkNSwePHigrM1M7ONXa33MA8CjpY0FxgDHCbpD8Dz+TAr+X1RLj8f2LmkfndgQY53rxBfRURcFRH9I6J/586dm3NZzMxsA1bThBkR50dE94ioJ3XmuTsivgiMB4bkYkOAcXl4PDBI0maSdiV17pmcD9u+JumA3Dv2xJI6ZmZm66yu1g1owiXAWElDgWeB4wEiYrqkscAMYDlwekS8m+ucBowEtgDuyC8zM7Nm0WYSZkRMAibl4ReAw5soNwwYViHeAPRuuRaamdnGrNbnMM3MzNYLTphmZmYFOGGamZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlaAE6aZmVkBTphmZmYFOGGamZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFeCEaWZmVoATppmZWQFOmGZmZgU4YZqZmRXghGlmZlaAE6aZmVkBTphmZmYFOGGamZkV4IRpZmZWQE0TpqSdJd0j6UlJ0yWdmeMdJU2Q9HR+376kzvmSZkmaKenIkvi+kqblcZdLUi2WyczMNky13sNcDpwTER8EDgBOl9QLOA+YGBE9gYn5M3ncIGAvYCBwhaT2eVojgFOBnvk1sDUXxDYsp5xyCl26dKF3794rYhdddBHdunWjX79+9OvXj9tvvx2At99+m5NPPpk+ffrQt29fJk2aBMBrr722omy/fv3o1KkTZ511Vg2WxsyaQ00TZkQsjIhH8vBrwJNAN+AYYFQuNgo4Ng8fA4yJiGURMQeYBewvqSvQISIejIgAri+pY1a1k046iTvvvHOV+Nlnn83UqVOZOnUqn/zkJwG4+uqrAZg2bRoTJkzgnHPO4b333mObbbZZUXbq1KnssssufPazn23V5TCz5lPrPcwVJNUDHwIeAnaMiIWQkirQJRfrBswrqTY/x7rl4fJ4pfmcKqlBUsPixYubdRlsw3HIIYfQsWPHQmVnzJjB4YcfDkCXLl3YbrvtaGhoWKnM008/zaJFizj44IObva1m1jraRMKUtDVwK3BWRLy6uqIVYrGa+KrBiKsion9E9O/cuXP1jbWN2m9/+1v23ntvTjnlFF566SUA+vbty7hx41i+fDlz5sxhypQpzJs3b6V6o0eP5oQTTsCn1s3WXzVPmJI2ISXLGyPiTzn8fD7MSn5flOPzgZ1LqncHFuR49wpxs2Zz2mmn8cwzzzB16lS6du3KOeecA6Tznd27d6d///6cddZZDBgwgLq6upXqjhkzhsGDB9ei2WbWTGrdS1bAtcCTEfGrklHjgSF5eAgwriQ+SNJmknYlde6ZnA/bvibpgDzNE0vqmDWLHXfckfbt29OuXTu+8pWvMHnyZADq6uoYPnw4U6dOZdy4cbz88sv07NlzRb3HHnuM5cuXs++++9aq6WbWDOrWXKRFHQR8CZgmaWqOfRe4BBgraSjwLHA8QERMlzQWmEHqYXt6RLyb650GjAS2AO7IL7Nms3DhQrp27QrAn//85xU9aJcuXUpEsNVWWzFhwgTq6uro1avXinqjR4/23qXZBqCmCTMi/k7l848AhzdRZxgwrEK8Aei9ag2z6g0ePJhJkyaxZMkSunfvzg9/+EMmTZrE1KlTkUR9fT1XXnklAIsWLeLII4+kXbt2dOvWjRtuuGGlaY0dO3bFJShmtv6q9R6mWZs0evToVWJDhw6tWLa+vp6ZM2c2Oa3Zs2c3W7vMrHZq3unHzMxsfeA9TFsvPPujPrVuwnqrxwXTat0Esw2C9zDNzMwKcMI0MzMrwAnTzMysACdMMzOzApwwzczMCnDCNDMzK8AJ08zMrAAnTDMzswKcMM3MzApwwjQzMyvACdPMzKwAJ0wzM7MCnDDNzMwKcMI0MzMrwAnTzMysACdMMzOzApwwzczMCnDCNDMzK8AJ08zMrAAnTDMzswKcMM3MzApwwjQzMyvACdPMzKwAJ0wzM7MCNqiEKWmgpJmSZkk6r9btMTOzDccGkzAltQd+B3wC6AUMltSrtq0yM7MNxQaTMIH9gVkRMTsi3gbGAMfUuE1mZraB2JASZjdgXsnn+TlmZma2zupq3YBmpAqxWKWQdCpwav74uqSZLdqqltcJWFLrRlSiS4fUugmtqc1uBy6s9K+xQWqz20Df3Gi2AbTh7YAKb4ddKgU3pIQ5H9i55HN3YEF5oYi4CriqtRrV0iQ1RET/WrdjY+ftUHveBm3DhrwdNqRDsg8DPSXtKmlTYBAwvsZtMjOzDcQGs4cZEcslfQP4G9AeuC4ipte4WWZmtoHYYBImQETcDtxe63a0sg3m8PJ6ztuh9rwN2oYNdjsoYpV+MWZmZlZmQzqHaWZm1mKcMNsYSTtImppfz0n6V8nnTQvU30nSLa3R1rZA0vckTZf0eF5HH27l+f8uz3eGpDdLttVxVUzjmtXdlUrS0Y23epR0bGvcwUpSd0njJD0t6RlJv5a0qaSTJP22pedflKSR1axrq46k15t5evWSnsjD/SR9sjmn39I2qHOYG4KIeAHoByDpIuD1iLi0ivoLgI3iC0TSgcCngX0iYpmkTsAaf1Ss5bzqImJ5eTwiTs/j64HbIqJftdOOiC+vYfx4/t3j+1jgNmBGtfMpSpKAPwEjIuKYfNvJq4BhgDvSWXPpB/RnPep34j3M9YCkwyU9KmmapOskbSZpv7xXtbmkrfJeVu+yX3DtJV2a6z0u6YxaL0sz6wosiYhlABGxJCIWSNpX0r2Spkj6m6Sukj4oaXJjxbyeHs/Dq5TP8UmSfirpXuDMpsqVk9RR0l/yOv+HpL0l1Ul6WNKhuczFkoaVzKd/Hh4o6RFJj0mamGMnSfqtpAHA0cAv8l7s+yU9UjLfnpKmNMN6PQx4KyJ+n9fru8DZwCnAlsDOku5UetDBhXneW0n6a273E5JOyPH9JD2Q45MlbZP/Ln+R18fjkr6ayx6a18Utkp6SdGNO3k1uoybW/9aSJub1OE3SMTleL+lJSVfn/5e7JG1R0s7HJT2Y29b4P7TSHrWk20q24QhJDXlaPywp88nc/r9LulzSbSXr6Lq83I82tmt9sIZtc4nSEZbHJV2aYyvt+atsT1XpaNmPgBPy3/IJrbk8ay0i/GqjL+Ai4PukW/7tkWPXA2fl4Z8Al5JuOn9+jtUDT+Th04Bbgbr8uWOtl6mZ18/WwFTgn8AVwEeBTYAHgM65zAmkS4zIZXfLw/+V1+3qyk8CrsjDTZarsN5/A1yYhw8DpubhvYAngY8DjwKblsynP9A5b+tdS7cXcBLw2zw8EjiuZL73AP3y8E+BM5phvX4TGF4h/mgetxDYAdgCeCK3/XPA1SVltyXt7c8G9suxDqSjWqcC38+xzYAGYFfgUOAV0k1H2gEPAh9ZwzZaaX3kWB3QIQ93AmaR7gRWDywvWV9jgS/m4SeAAXn4kpJtuWLd58+3AYeWbZ/2eRvuDWxetg1Hk448NG6fxvltR/q73arW/0dr+Ft4Pb83tW06AjP5dwfS7Zr4O22cTn1T63Z9ePmQbNvXHpgTEf/Mn0cBpwOXkX6hPQy8RfoiK3cE8N+RDyVGxIst3tpWFBGvS9oXOBj4GHAT6UdEb2BC/gHcnvQFD+kL8j9JX4gn5NeeqylPniYFypX6CCmBEBF3K52X3jYipku6Afgf4MBIDwkodQBwX0TMyXWLbK9rgJMlfSsvz/4F6qyJqHBbyZL4hEinDpD0J9Ly3g5cKulnpATx/yT1ARZGxMMAEfFqrvMfwN4leyDbAj2Bt4HJETE/l5tK+oJ9meLrvrGdP5V0CPAe6Z7SO+ZxcyJiah6eAtRL2g7YJiIeyPE/kg71r8l/Kt1qs450tKMXKZnMbtyGpITZeCvO/wCOlvTt/HlzoAfpR9T6oNK2+Qfp++caSX8l/aDYYDlhtn1vrGZcR9Je1iakf77ysk198W0wIh0unARMkjSN9GNiekQcWKH4TcDN+Us+IuLp/KXeVHn49zrVGsqVWt19jfuQEsCOFcqszfa6FbgQuBuY0pjI1tF0csJf0TCpA+nWk+9WaGNExD/zj5dPAhdLugv4S4WykJbzjIj4W9k8DgWWlYTeJX1HVbPuAb5A2lvfNyLekTSX9P9BhelvQeXt1Wg5K5+62jy3dVfg26S955ckjczjVjctAZ+LiPX1/tWrbJtIN4zZHzicdHe1b5COqqxYb/nQbYv0LWhtPofZ9m1O+hW8e/78JeDePHwV8APgRuBnFereBXxNUh2kc2st3NZWJWlPST1LQv1Iv9Y7K3UIQtImkvYCiIhnSP/oP+Dfe44zmypfpmg5gPtIX9qNSWBJRLwq6bOkQ5mHAJfnPZtSDwIfzV/GTW2v14BtGj9ExFuku1uNAH7fRHuqNRHYUtKJuR3tgV+SDrMtBT6udJ52C1InpPsl7QQsjYg/kE4T7AM8Bewkab88nW3y3+LfgNMkbZLje0jaajXtqWbdQ9pjXZST5cdo4kbajSLiJeA1SQfk0KCS0XOBfpLaSdqZf+/BdyD9mHpF0o6k5/CSl3k3pU5gkPb6G/0NOKPk3N+HVteu9YGkrYFtI9005ixyh0XSets3Dx9D+lFfbqW/5fWBE2bb9xZwMmnPaBrpENN/5y+z5RHxR9Ihxv0kHVZW9xrgWeBxSY8Bn2/FdreGrYFRjR0OSIfELiD1Ev5ZXuapwICSOjcBXyQdniUfFl1deaopl10E9M9tugQYotSD9xJgaD68/lvg12XzWEw6fPenPI+bWNUY4Du508j7c+xG0p7cXU20pyqRTjB9Bjhe0tOkc21vAd/NRf4O3EBaB7dGRANpz3lyPlT3PeAneZ2dAPwmL88E0g/Aa0i9fB9R6lxzJas52lVg3V8paX5+PUhaH/0lNZB+uDxVYLGHAlfl+iKdrwO4H5gDTCP9EHgkt+kx0jnd6cB1uRwR8SbwdeBOSX8Hni+Z1o9JiePxvNw/LtCutm4b4Lb8t34vqXMYwNWkH3+TgQ9T+UjZPUCv9anTj+/0Y7aey+fEto2IH9S6LesrSVtHxOt5+Dyga0ScuS7TynuSvwOejojhzdhcqxGfwzRbj0n6M/B+0nkjW3ufknQ+6Tvx/0g9ONfWVyQNIZ23e5S0B20bAO9hmpmZFeBzmGZmZgU4YZqZmRXghGlmZlaAE6bZekrSdyVdU6Dcf0tq8R60kqLkemGzDY47/Zi1Eq18A+otSXdOeTd//mpE3NgKbTgU+ENEdG+BaQfQMyJmNfe0zdoCX1Zi1koiYuvG4Xy7ti9HxP/WrkVmVg0fkjWrMaXHtV0maUF+XZZjm+a7oJyRy7WXdL+kC/LniyT9oWQ6H1F6lNbLkuZJOinHR0r6Sb793B2k29W9nl87SVoqaYeS6ewraXHjrevK2to+Hwp+RtJrSo/b2rlCuU/luxG9mttyUcm4zSX9QdILua0P59vLNT5Oa3ae9hxJX2iu9Wy2rpwwzWrve6QnlfQD+pLuV/r9fEu4LwI/kvRB4DzSkzqGlU9AUg9SMvwN6cbj/Ui3kFshIt4g3fN0QURsnV8LSDev/8+Sol8ExkTEOxXa+i1gMOkm6x1Iz8hcWqHcG8CJpMdYfYp079hj87ghpPu97ky6t+7XgDdzQr8c+EREbEO6/d1Ky2BWS06YZrX3BeBHEbEo30/2h6Sb7BMRT5AeWfZn0tMxvpSf0FJpGv8bEaMj4p2IeKHkMVZrMoqUJBtvtD6YdK/YSr5MSuYzI3ms0hNSImJSREyLiPci4nHSY64+mke/Q0qUu0fEuxExpfHRX6R7JfeWtEVELIyI6QWXwazFOWGa1d5OpNuxNfq/HGs0ivTswdsj4ukmprEz8Mxazn8c6SbYu5Eebv1KRExel/lI+rCke/Kh3VdIe5Gd8ugbSE/uGJMPQf9c0iZ5D/iEXHahpL9K+sBaLpNZs3PCNKu9Baz8CKoeOdboCtKDeY+U9JEmpjGPdE/ZNVmlW3x+RNhY0l7ql2h677Ka+fwRGA/sHBHbAv9NflZk3gP+YUT0Ih12/TTp8C0R8beI+DjpgcxPkZ56YdYmOGGa1d5o4PuSOufHgF0A/AFA0pdIzxU8Cfgm6XFmW1eYxo3AEZL+U1KdpB0k9atQ7nlgB0nblsWvz/M4unHeTbgG+LGknkr2Lu0wVGIb4MWIeEvpAcMrHi0n6WOS+uTDv6+SDtG+K2lHSUfnc5nLgNf592U3ZjXnhGlWez8BGoDHSc9dfAT4Se7IcxlwYkS8np992gCs8qioiHiW1BHnHOBFUmeZvhXKPUVK0LNzD9Wdcvx+0vnDRyJi7mra+ivS3uhdpGR3LbBFhXJfJ3VWeo30A2Bsybj3Abfk+k+SnqP4B9L30TmkvesXSec8v76atpi1Kt+4wMwAkHQ38MeIWOPdg8w2Rk6YZoak/YAJpHOOr9W6PWZtkQ/Jmm3kJI0C/hc4y8nSrGnewzQzMyvAe5hmZmYFOGGamZkV4IRpZmZWgBOmmZlZAU6YZmZmBThhmpmZFfD/AevDZOM7qTIwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a chart with the following size\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "# Plot a bar chart using the index (category values) and the count of each category. \n",
    "# To make the bars more translucent, modify alpha value\n",
    "ax = sns.barplot(df_count.index, df_count.values, alpha=1)\n",
    "\n",
    "plt.title(\"Comments per class\\n\", fontsize=20)\n",
    "plt.ylabel('Occurrences', fontsize=12)\n",
    "plt.xlabel('Toxicity class', fontsize=12)\n",
    "\n",
    "# Adding the text labels for each bar\n",
    "rects = ax.patches\n",
    "labels = df_count.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 10, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.0344774 ,  1.24177721,  6.13423946,  5.83098767])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowdata = len(df)\n",
    "\n",
    "pctdata = df_count.values / rowdata * 100\n",
    "pctdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This concludes that the percentage of toxicity classes\n",
    "    - Toxicity: n%\n",
    "    - Severe Toxicity: n%\n",
    "    - Obscene Language: n%\n",
    "    - Insult: n%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maltesers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-download stopwords for backup\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Setup stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TextComment</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>SevereToxicity</th>\n",
       "      <th>ObsceneLanguage</th>\n",
       "      <th>Insult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>go back to your country smelly arab</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>russian why u mad bitch</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man  i m really not trying to edit war  it...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>china teammate so toxic</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you  sir  are my hero  any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128601</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>baiter fucking idiot</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128602</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>you should be ashamed of yourself  that is a h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128603</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>fking noob</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128604</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128605</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>and     i really don t think you understand ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128606 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                        TextComment  \\\n",
       "0       0000997932d777bf                go back to your country smelly arab   \n",
       "1       000103f0d9cfb60f                            russian why u mad bitch   \n",
       "2       000113f07ec002fd  hey man  i m really not trying to edit war  it...   \n",
       "3       0001b41b1c6bb37e                            china teammate so toxic   \n",
       "4       0001d958c54c6e35  you  sir  are my hero  any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "128601  ffe987279560d7ff                               baiter fucking idiot   \n",
       "128602  ffea4adeee384e90  you should be ashamed of yourself  that is a h...   \n",
       "128603  ffee36eab5c267c9                                         fking noob   \n",
       "128604  fff125370e4aaaf3  and it looks like it was actually you who put ...   \n",
       "128605  fff46fc426af1f9a    and     i really don t think you understand ...   \n",
       "\n",
       "        Toxic  SevereToxicity  ObsceneLanguage  Insult  \n",
       "0           0               1                0       1  \n",
       "1           0               1                1       1  \n",
       "2           0               0                0       0  \n",
       "3           1               0                0       1  \n",
       "4           0               0                0       0  \n",
       "...       ...             ...              ...     ...  \n",
       "128601      0               1                1       1  \n",
       "128602      0               0                0       1  \n",
       "128603      0               1                1       1  \n",
       "128604      0               0                0       0  \n",
       "128605      0               0                0       0  \n",
       "\n",
       "[128606 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers, capital letters, punctuation, '\\n'\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Remove all random numbers with attached letters\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "# '[%s]' % re.escape(string.punctuation),' ' - replace punctuation with white space\n",
    "# .lower() - Convert all strings to lowercase \n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "# Remove all '\\n' in the string and replace it with a space\n",
    "filter_line = lambda x: re.sub(\"\\n\", \" \", x)\n",
    "\n",
    "# Remove all Non-ASCII characters \n",
    "filter_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ', x)\n",
    "\n",
    "# Apply all the lambda functions wrote previously through .map on the comments column\n",
    "df['TextComment'] = df['TextComment'].map(alphanumeric).map(punc_lower).map(filter_line).map(filter_non_ascii)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'your new user tags   hey man   i just noticed that you put a little tag on your user page saying that vandals dont like you  that is most certainly not the case    well  at least not as far as i m concerned  the only reason that i keep screwing around with you is because you re the only admin that s banned me who has a memorable  easy to spell name  honestly  it s nothing personal   just trying to put a smile on your face '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View comment\n",
    "df['TextComment'][50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new dataframe\n",
    "df.to_csv(os.path.join('train_colab', 'u_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleansed dataset\n",
    "#os.path.join('train_colab', 'u_train.csv')\n",
    "#df = pd.read_csv(os.path.join('train_colab', 'u_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df['TextComment']\n",
    "y = df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'TextComment', 'Toxic', 'SevereToxicity',\n",
       "       'ObsceneLanguage', 'Insult'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxic</th>\n",
       "      <th>SevereToxicity</th>\n",
       "      <th>ObsceneLanguage</th>\n",
       "      <th>Insult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128601</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128602</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128603</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128604</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128605</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128606 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Toxic  SevereToxicity  ObsceneLanguage  Insult\n",
       "0           0               1                0       1\n",
       "1           0               1                1       1\n",
       "2           0               0                0       0\n",
       "3           1               0                0       1\n",
       "4           0               0                0       0\n",
       "...       ...             ...              ...     ...\n",
       "128601      0               1                1       1\n",
       "128602      0               0                0       1\n",
       "128603      0               1                1       1\n",
       "128604      0               0                0       0\n",
       "128605      0               0                0       0\n",
       "\n",
       "[128606 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 350000 # number of words in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=MAX_WORDS,\n",
    "                               output_sequence_length=1800,\n",
    "                               output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128606"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer.adapt(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorized_text = vectorizer(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([376, 698], dtype=int64)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer('Hello, test')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128606, 1800), dtype=int64, numpy=\n",
       "array([[  112,   155,     3, ...,     0,     0,     0],\n",
       "       [ 1117,    76,   188, ...,     0,     0,     0],\n",
       "       [  322,   367,     4, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [25107,  6365,     0, ...,     0,     0,     0],\n",
       "       [    6,    11,   525, ...,     0,     0,     0],\n",
       "       [    6,     4,   123, ...,     0,     0,     0]], dtype=int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Particularly helpful if you have a large dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# This is a data pipeline\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# MCShBP - map, cache, shuffle, batch, prefetch  from_tensor_slices, list_file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m140000\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:809\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    733\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4551\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   4550\u001b[0m   \u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4551\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4552\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   4553\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:125\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    122\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    124\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 125\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1640\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1631\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1632\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1633\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1636\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1637\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1640\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1643\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# Particularly helpful if you have a large dataset\n",
    "# This is a data pipeline\n",
    "# MCShBP - map, cache, shuffle, batch, prefetch  from_tensor_slices, list_file\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X.values, y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(140000)\n",
    "dataset = dataset.batch(16)\n",
    "\n",
    "# Helps prevent bottlenecks\n",
    "dataset = dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.data.Dataset.list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training partition (60%)\n",
    "train = dataset.take(int(len(dataset)*0.6))\n",
    "\n",
    "# Validation partition, skip 60%(Train), take 20%(Validation)\n",
    "val = dataset.skip(int(len(dataset)*0.6)).take(int(len(dataset)*0.2))\n",
    "\n",
    "#Testing partition, skip 80%(Training+Validation), take 10%(Test)\n",
    "test = dataset.skip(int(len(dataset)*0.8)).take(int(len(dataset)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_train = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_train.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bidirectional??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LSTM??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dense??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Create the embedding layer\n",
    "model.add(Embedding(MAX_WORDS+1, 32))\n",
    "\n",
    "# Bidirectional is a modifier and will allow us to pass features from our LSTM to our sequence model\n",
    "# Bidirectional allows the information to be parsed backwards and forward the LSTM layer\n",
    "# Bidirectional layer is helpful for double negative statements\n",
    "# 32 different LSTM units\n",
    "# GPU Acceleration required is tanh, dictated by tensorflow\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "\n",
    "# Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# 6 Final units in dense layer \n",
    "# Using sigmoid transforms data to 0 or 1\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Summarizes the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using binary crossentropy instead of categorical crossentropy\n",
    "# Because we have a multiple output model, so we reduce the loss\n",
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model and displaying chart containing Loss and Validation Loss\n",
    "**_val_loss_** is the value of cost function for your cross-validation data. <br>\n",
    "**_loss_** is the value of cost function for your training data.\n",
    "\n",
    "On validation data, neurons using drop out do not drop random neurons. The reason is that during training we use drop out in order to add some noise for avoiding over-fitting. During calculating cross-validation, we are in the recall phase and not in the training phase. We use all the capabilities of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the dataset\n",
    "# Epoch = how many passes for dataset\n",
    "batch_size=64\n",
    "archive = model.fit(train, epochs=1, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "pd.DataFrame(archive.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model after training, load model function(H5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data after training Epoch1_2:21am 19/July/2022\n",
    "model.save('epoch1_2-dense-128-256_trainer.h5')\n",
    "# Load model data\n",
    "model = tf.keras.models.load_model('epoch1_2-dense-128-256_trainer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = vectorizer('china teammate noob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(input_text,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = test.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, batch_y = test.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(model.predict(batch_X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(np.expand_dims(input_text,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator(): \n",
    "    \n",
    "    # Unpack the batch \n",
    "    X_true, y_true = batch\n",
    "    \n",
    "    # Make a prediction \n",
    "    yhat = model.predict(X_true)\n",
    "    \n",
    "    # Flatten the predictions\n",
    "    y_true = y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    \n",
    "    pre.update_state(y_true, yhat)\n",
    "    re.update_state(y_true, yhat)\n",
    "    acc.update_state(y_true, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Precision: {pre.result().numpy()},Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test and Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gradio jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_str = vectorizer('fucking noob china man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model.predict(np.expand_dims(input_str,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_comment(Chat):\n",
    "    vectorized_comment = vectorizer([Chat])\n",
    "    results = model.predict(vectorized_comment)\n",
    "    \n",
    "    text = ''\n",
    "    for idx, col in enumerate(df.columns[2:]):\n",
    "        text += '{}: {}\\n'.format(col, results[0][idx]>0.5)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interface = gr.Interface(fn=score_comment, \n",
    "                         inputs=gr.inputs.Textbox(lines=2, placeholder='Type to evaluate'),\n",
    "                        outputs='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interface.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
